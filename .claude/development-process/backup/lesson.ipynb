{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Attention Mechanism - Interactive Learning Notebook\n",
    "\n",
    "Welcome to the interactive attention mechanism tutorial! This notebook will guide you through implementing the core components of the attention mechanism step by step.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand and implement:\n",
    "1. **Linear Projections** for Query (Q), Key (K), and Value (V) matrices\n",
    "2. **Scaled Dot-Product Attention** computation\n",
    "3. **Softmax & Attention Weights** calculation\n",
    "4. **Value Aggregation** using attention weights\n",
    "\n",
    "## Example Prompt\n",
    "Throughout this tutorial, we'll use this consistent example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_EXAMPLE = \"The cat sat on the mat\"\n",
    "print(f\"Working with example: '{PROMPT_EXAMPLE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.visualizations import (\n",
    "    visualize_qkv_projections,\n",
    "    visualize_attention_scores,\n",
    "    visualize_attention_weights,\n",
    "    visualize_attended_values\n",
    ")\n",
    "from src.model_utils import tokenize_text, create_embeddings\n",
    "from src.evaluation import evaluate_attention_output\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Setup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Linear Projections (Q, K, V)\n",
    "\n",
    "## The Intuition: Three Perspectives on Information\n",
    "\n",
    "Imagine you're at a library looking for information about \"machine learning books.\" You would:\n",
    "1. **Ask** the librarian about books on machine learning (Query - what you're looking for)\n",
    "2. The librarian **checks** the catalog for available books (Keys - what's available to match)\n",
    "3. You **receive** the actual books that match (Values - the information you get)\n",
    "\n",
    "In attention mechanisms, we create these three \"perspectives\" for each word in our sentence.\n",
    "\n",
    "## Theory: Why Do We Need Q, K, V?\n",
    "\n",
    "The attention mechanism needs to answer: **\"For each word, which other words should it pay attention to?\"**\n",
    "\n",
    "Consider our example: **\"The cat sat on the mat\"**\n",
    "\n",
    "For the word \"cat\":\n",
    "- **Query (Q)**: \"What information does 'cat' need?\" → Maybe it needs to know what action it's performing\n",
    "- **Key (K)**: \"What information can each word provide?\" → \"sat\" can provide action information  \n",
    "- **Value (V)**: \"What is the actual information?\" → The semantic meaning of \"sat\" (an action)\n",
    "\n",
    "### The Three Transformations\n",
    "\n",
    "Starting with the same input embeddings **X**, we create three different \"views\":\n",
    "\n",
    "- **Query (Q)**: *\"What am I looking for?\"* - Transforms input to represent information needs\n",
    "- **Key (K)**: *\"What can I provide?\"* - Transforms input to represent available information  \n",
    "- **Value (V)**: *\"What information do I actually contain?\"* - Transforms input to represent the content to be retrieved\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "$$Q = XW_Q$$\n",
    "$$K = XW_K$$  \n",
    "$$V = XW_V$$\n",
    "\n",
    "Where:\n",
    "- $X \\in \\mathbb{R}^{L \\times d_{model}}$: Input embeddings (sequence length × embedding dimension)\n",
    "- $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$: Learned weight matrices (embedding dim × projection dim)\n",
    "- $Q, K, V \\in \\mathbb{R}^{L \\times d_k}$: Projected query, key, value matrices\n",
    "\n",
    "### Why Different Weight Matrices?\n",
    "\n",
    "Each weight matrix learns to extract different aspects:\n",
    "- $W_Q$: Learns to extract \"what information this position needs\"\n",
    "- $W_K$: Learns to extract \"what information this position can provide\" \n",
    "- $W_V$: Learns to extract \"the actual information content\"\n",
    "\n",
    "### Tensor Shape Deep Dive\n",
    "\n",
    "For \"The cat sat on the mat\" (6 tokens):\n",
    "- Input embeddings: `(1, 6, 512)` → 1 batch, 6 tokens, 512-dim embeddings\n",
    "- After projection: `(1, 6, 64)` → 1 batch, 6 tokens, 64-dim projections\n",
    "\n",
    "The reduction from 512 to 64 dimensions serves two purposes:\n",
    "1. **Computational efficiency**: Smaller attention computations\n",
    "2. **Multiple heads**: We can have multiple attention heads in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize example data\n",
    "PROMPT_EXAMPLE = \"The cat sat on the mat\"\n",
    "tokens = tokenize_text(PROMPT_EXAMPLE)\n",
    "embeddings = create_embeddings(tokens)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED: Implement linear projections for Q, K, V\n",
    "# Implemented: Create the linear projection layers and compute Q, K, V matrices\n",
    "\n",
    "def create_qkv_projections(embeddings, d_model=512, d_k=64):\n",
    "    \"\"\"\n",
    "    Create Query, Key, and Value projections from input embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Input embeddings tensor (batch_size, seq_len, d_model)\n",
    "        d_model: Dimension of input embeddings\n",
    "        d_k: Dimension of Q, K, V projections\n",
    "    \n",
    "    Returns:\n",
    "        Q, K, V: Query, Key, Value tensors (batch_size, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    # Implementation follows the mathematical foundation:\n",
    "    # Q = X * W_Q, K = X * W_K, V = X * W_V\n",
    "    \n",
    "    # 1. Create linear projection layers for Q, K, V\n",
    "    # Each projects from d_model (512) to d_k (64) dimensions\n",
    "    \n",
    "    # 2. Apply the projections to the input embeddings\n",
    "    \n",
    "    # 3. Return Q, K, V tensors\n",
    "\n",
    "# Test implementation\n",
    "Q, K, V = create_qkv_projections(embeddings)\n",
    "print(f\"Q shape: {Q.shape}, K shape: {K.shape}, V shape: {V.shape}\")\n",
    "print(\"Successfully created Q, K, V projections!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Q, K, V Projections\n",
    "# This will be populated once you implement the function above\n",
    "# visualize_qkv_projections(Q, K, V, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Scaled Dot-Product Attention\n",
    "\n",
    "## The Intuition: Measuring Compatibility\n",
    "\n",
    "Think of this step as **matchmaking between questions and answers**:\n",
    "- Each Query asks: *\"What information do I need?\"*\n",
    "- Each Key responds: *\"Here's what I can provide\"*  \n",
    "- The dot product measures: *\"How well do they match?\"*\n",
    "\n",
    "### Why Dot Product for Similarity?\n",
    "\n",
    "The dot product between two vectors measures their **alignment**:\n",
    "- **High dot product**: Vectors point in similar directions → High compatibility\n",
    "- **Low dot product**: Vectors are orthogonal → Low compatibility  \n",
    "- **Negative dot product**: Vectors point in opposite directions → Incompatible\n",
    "\n",
    "**Example**: If Query for \"cat\" is looking for \"action information\" and Key for \"sat\" provides \"action information\", their dot product will be high.\n",
    "\n",
    "### The Mathematical Operation\n",
    "\n",
    "$$\\text{Attention Scores} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "Let's break this down step by step:\n",
    "\n",
    "#### Step 1: Matrix Multiplication $QK^T$\n",
    "- $Q \\in \\mathbb{R}^{L \\times d_k}$: Each row is a query vector for one token\n",
    "- $K^T \\in \\mathbb{R}^{d_k \\times L}$: Each column is a key vector for one token  \n",
    "- Result: $\\mathbb{R}^{L \\times L}$ matrix where entry $(i,j)$ = similarity between token $i$'s query and token $j$'s key\n",
    "\n",
    "#### Step 2: Scaling by $\\sqrt{d_k}$\n",
    "\n",
    "**Why do we need scaling?**\n",
    "As the dimension $d_k$ increases, dot products tend to grow larger in magnitude. This pushes values toward the extremes of the softmax function where gradients become extremely small.\n",
    "\n",
    "**The Problem**: Without scaling, for $d_k = 512$:\n",
    "- Random dot products have variance ≈ 512\n",
    "- Softmax becomes nearly deterministic (almost one-hot)\n",
    "- Gradients vanish during training\n",
    "\n",
    "**The Solution**: Dividing by $\\sqrt{d_k}$ normalizes the variance back to ≈ 1\n",
    "\n",
    "### Tensor Shape Analysis\n",
    "\n",
    "For \"The cat sat on the mat\" (6 tokens, $d_k = 64$):\n",
    "\n",
    "1. **Q shape**: `(1, 6, 64)` - 6 query vectors, each 64-dimensional\n",
    "2. **K shape**: `(1, 6, 64)` - 6 key vectors, each 64-dimensional  \n",
    "3. **K^T shape**: `(1, 64, 6)` - Transposed for matrix multiplication\n",
    "4. **QK^T shape**: `(1, 6, 6)` - 6×6 attention score matrix\n",
    "\n",
    "Each element `[i, j]` represents: *\"How much should token i attend to token j?\"*\n",
    "\n",
    "### Attention Score Matrix Interpretation\n",
    "\n",
    "For our example sentence, the 6×6 matrix might look like:\n",
    "```\n",
    "         The  cat  sat  on  the  mat\n",
    "    The  [ ?   ?    ?   ?   ?    ? ]\n",
    "    cat  [ ?   ?    ?   ?   ?    ? ]  \n",
    "    sat  [ ?   ?    ?   ?   ?    ? ]\n",
    "    on   [ ?   ?    ?   ?   ?    ? ]\n",
    "    the  [ ?   ?    ?   ?   ?    ? ]\n",
    "    mat  [ ?   ?    ?   ?   ?    ? ]\n",
    "```\n",
    "\n",
    "Higher scores indicate stronger relationships (e.g., \"cat\" → \"sat\" for subject-verb relationship).\n",
    "\n",
    "### The Complete Formula Intuition\n",
    "\n",
    "$$\\text{Score}_{i,j} = \\frac{\\text{query}_i \\cdot \\text{key}_j}{\\sqrt{d_k}}$$\n",
    "\n",
    "This answers: *\"How relevant is the information that token j can provide to what token i is looking for?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED: Implement scaled dot-product attention scores\n",
    "# Implemented: Compute the attention scores using the scaled dot-product formula\n",
    "\n",
    "def compute_attention_scores(Q, K):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention scores.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch_size, seq_len, d_k)\n",
    "        K: Key tensor (batch_size, seq_len, d_k)\n",
    "    \n",
    "    Returns:\n",
    "        attention_scores: Attention scores (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Implementation follows: scores = Q * K^T / sqrt(d_k)\n",
    "    \n",
    "    # 1. Get the dimension d_k for scaling\n",
    "    \n",
    "    # 2. Compute dot product between Q and K^T\n",
    "    # Q shape: (batch_size, seq_len, d_k)\n",
    "    # K^T shape: (batch_size, d_k, seq_len) - transpose last two dimensions\n",
    "    # Result: (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # 3. Scale by sqrt(d_k) to prevent extreme values\n",
    "    \n",
    "\n",
    "# Test implementation\n",
    "attention_scores = compute_attention_scores(Q, K)\n",
    "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
    "print(f\"d_k = {Q.shape[-1]}, scaling factor = {torch.sqrt(torch.tensor(Q.shape[-1], dtype=torch.float32)):.2f}\")\n",
    "print(\"Successfully computed attention scores!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Attention Scores\n",
    "# This will be populated once you implement the function above\n",
    "# visualize_attention_scores(attention_scores, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Softmax & Attention Weights\n",
    "\n",
    "## The Intuition: From Scores to Decisions\n",
    "\n",
    "Imagine you're deciding how to allocate your attention while reading \"The cat sat on the mat\":\n",
    "- You have **compatibility scores** for how relevant each word is\n",
    "- But you need to make a **decision**: How much attention to give each word?\n",
    "- Softmax converts raw scores into a **probability distribution** - a recipe for attention allocation\n",
    "\n",
    "### Why Convert to Probabilities?\n",
    "\n",
    "Raw attention scores can be any real numbers (positive, negative, large, small). We need:\n",
    "1. **Interpretability**: Weights between 0 and 1 are easier to understand\n",
    "2. **Normalization**: Weights sum to 1, so we're not \"over-attending\"  \n",
    "3. **Differentiability**: Smooth function for gradient-based learning\n",
    "\n",
    "### The Softmax Function\n",
    "\n",
    "$$\\text{Attention Weight}_{i,j} = \\frac{\\exp(\\text{Score}_{i,j})}{\\sum_{k=1}^{L} \\exp(\\text{Score}_{i,k})}$$\n",
    "\n",
    "**What this does**:\n",
    "- **Exponential**: $\\exp(x)$ makes all values positive and amplifies differences\n",
    "- **Normalization**: Division ensures weights sum to 1 for each query position\n",
    "- **Probability distribution**: Each row becomes a valid probability distribution\n",
    "\n",
    "### Step-by-Step Example\n",
    "\n",
    "For \"The cat sat on the mat\", let's say token \"cat\" has attention scores:\n",
    "```\n",
    "Raw scores:     [0.1, 0.8, 1.2, 0.3, 0.1, 0.4]\n",
    "After exp():    [1.11, 2.23, 3.32, 1.35, 1.11, 1.49]\n",
    "Sum:            11.61\n",
    "After softmax:  [0.09, 0.19, 0.29, 0.12, 0.09, 0.13]\n",
    "```\n",
    "\n",
    "**Interpretation**: \"cat\" should pay:\n",
    "- 29% attention to \"sat\" (highest score → highest weight)\n",
    "- 19% attention to itself  \n",
    "- 13% attention to \"mat\"\n",
    "- etc.\n",
    "\n",
    "### The Attention Matrix\n",
    "\n",
    "After applying softmax to all rows, we get a **stochastic matrix**:\n",
    "\n",
    "$$\\text{Attention}_{6 \\times 6} = \\begin{bmatrix}\n",
    "\\text{The→The} & \\text{The→cat} & \\text{The→sat} & \\cdots \\\\\n",
    "\\text{cat→The} & \\text{cat→cat} & \\text{cat→sat} & \\cdots \\\\\n",
    "\\text{sat→The} & \\text{sat→cat} & \\text{sat→sat} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Properties**:\n",
    "- Each row sums to 1 (probability distribution)\n",
    "- Each entry is between 0 and 1\n",
    "- Row $i$ shows how token $i$ distributes its attention\n",
    "\n",
    "### Concrete Example: \"The cat sat on the mat\"\n",
    "\n",
    "The attention weights might reveal linguistic patterns:\n",
    "```\n",
    "         The   cat   sat   on    the   mat\n",
    "    The [0.2, 0.15, 0.1, 0.15, 0.25, 0.15]  # Articles attend to nouns\n",
    "    cat [0.1, 0.25, 0.4, 0.05, 0.05, 0.15]  # Subject attends to verb\n",
    "    sat [0.05, 0.35, 0.3, 0.1, 0.05, 0.15]  # Verb attends to subject\n",
    "    on  [0.1, 0.1, 0.15, 0.2, 0.15, 0.3]   # Preposition attends to object\n",
    "    the [0.15, 0.1, 0.1, 0.15, 0.25, 0.25]  # Article attends to noun\n",
    "    mat [0.1, 0.2, 0.15, 0.25, 0.15, 0.15]  # Object attends to preposition\n",
    "```\n",
    "\n",
    "**Key Insights**:\n",
    "- \"cat\" (row 2) has highest weight 0.4 for \"sat\" → Subject-verb relationship\n",
    "- \"on\" (row 4) has highest weight 0.3 for \"mat\" → Preposition-object relationship\n",
    "- Self-attention captures word importance in context\n",
    "\n",
    "### Mathematical Properties\n",
    "\n",
    "1. **Row-wise normalization**: $\\sum_{j=1}^{L} \\text{Attention}_{i,j} = 1$ for all $i$\n",
    "\n",
    "2. **Temperature effect**: Higher scores get exponentially more weight\n",
    "   - Score difference of 1 → Weight ratio of $e ≈ 2.7$\n",
    "   - Score difference of 2 → Weight ratio of $e^2 ≈ 7.4$\n",
    "\n",
    "3. **Concentration**: Softmax concentrates probability mass on highest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED: Implement softmax to get attention weights\n",
    "# Implemented: Apply softmax to convert attention scores to attention weights\n",
    "\n",
    "def compute_attention_weights(attention_scores):\n",
    "    \"\"\"\n",
    "    Convert attention scores to attention weights using softmax.\n",
    "    \n",
    "    Args:\n",
    "        attention_scores: Attention scores (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Implementation follows: weights = softmax(scores)\n",
    "    \n",
    "    # Apply softmax along the last dimension (key positions)\n",
    "    # This ensures each query position has attention weights that sum to 1\n",
    "    # dim=-1 means we normalize across the key dimension for each query\n",
    "\n",
    "# Test implementation\n",
    "attention_weights = compute_attention_weights(attention_scores)\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Sum of weights for first query position: {attention_weights[0, 0, :].sum():.4f}\")\n",
    "print(f\"Sum of weights for second query position: {attention_weights[0, 1, :].sum():.4f}\")\n",
    "print(\"All attention weights should sum to 1.0 for each query position\")\n",
    "print(\"Successfully computed attention weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Attention Weights\n",
    "# This will be populated once you implement the function above\n",
    "# visualize_attention_weights(attention_weights, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Value Aggregation\n",
    "\n",
    "## The Intuition: Gathering Information\n",
    "\n",
    "Now comes the payoff! We've decided **where** to look (attention weights), now we need to **gather** the actual information from those locations. This is like:\n",
    "\n",
    "- **Step 3 of our library analogy**: After deciding which books are most relevant (attention weights), you actually **read and combine** information from those books (values)\n",
    "- **Weighted averaging**: Instead of reading all books equally, you focus more on the most relevant ones\n",
    "\n",
    "### The Mathematical Operation\n",
    "\n",
    "$$\\text{Output} = \\text{Attention Weights} \\times V$$\n",
    "\n",
    "More precisely:\n",
    "$$\\text{Output}_i = \\sum_{j=1}^{L} \\text{Attention}_{i,j} \\times V_j$$\n",
    "\n",
    "Where:\n",
    "- $\\text{Output}_i$: The new representation for token $i$\n",
    "- $\\text{Attention}_{i,j}$: How much token $i$ attends to token $j$  \n",
    "- $V_j$: The value vector for token $j$\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "For each token, we create a **personalized summary** of the entire sequence:\n",
    "\n",
    "**For token \"cat\" in \"The cat sat on the mat\":**\n",
    "```\n",
    "Original value of \"cat\": [cat's semantic features]\n",
    "After attention:         [0.1×\"The\" + 0.25×\"cat\" + 0.4×\"sat\" + 0.05×\"on\" + 0.05×\"the\" + 0.15×\"mat\"]\n",
    "```\n",
    "\n",
    "**The result**: \"cat\" now contains:\n",
    "- 40% of \"sat\"'s information (strong subject-verb connection)  \n",
    "- 25% of its own information (self-context)\n",
    "- 15% of \"mat\"'s information (object relationship)\n",
    "- Small amounts from other tokens\n",
    "\n",
    "### What Makes This Powerful?\n",
    "\n",
    "1. **Contextualization**: Each token's representation now includes relevant context\n",
    "2. **Selective Focus**: More important relationships get more weight\n",
    "3. **Information Flow**: Semantic information flows from keys to queries through values\n",
    "\n",
    "### Tensor Shape Analysis\n",
    "\n",
    "For \"The cat sat on the mat\" (6 tokens, $d_k = 64$):\n",
    "\n",
    "1. **Attention weights**: `(1, 6, 6)` - How each token attends to every other token\n",
    "2. **Values (V)**: `(1, 6, 64)` - 64-dimensional value vector for each token\n",
    "3. **Output**: `(1, 6, 64)` - 64-dimensional attended representation for each token\n",
    "\n",
    "**Matrix multiplication**:\n",
    "- Row $i$ of attention weights: `(1, 6)` - attention distribution for token $i$\n",
    "- Full values matrix: `(6, 64)` - all value vectors\n",
    "- Result for token $i$: `(1, 64)` - weighted combination of all value vectors\n",
    "\n",
    "### The Complete Information Flow\n",
    "\n",
    "Let's trace what happens to the word \"cat\":\n",
    "\n",
    "1. **Query Creation**: \"cat\" → Query vector (what information does \"cat\" need?)\n",
    "2. **Attention Computation**: Query compared to all Key vectors → Attention scores  \n",
    "3. **Softmax**: Attention scores → Attention weights (probability distribution)\n",
    "4. **Value Aggregation**: Attention weights × Value vectors → Final representation\n",
    "\n",
    "**The result**: The new representation of \"cat\" contains:\n",
    "- Its original semantic information\n",
    "- **Plus** contextual information from \"sat\" (it performs this action)\n",
    "- **Plus** contextual information from \"mat\" (location relationship)  \n",
    "- **Plus** smaller amounts from other tokens\n",
    "\n",
    "### Why Values Are Different From Keys?\n",
    "\n",
    "- **Keys**: Optimized to be \"found\" by queries (searchable representations)\n",
    "- **Values**: Optimized to provide useful information (retrievable content)\n",
    "- **Analogy**: Keys are like book titles/tags, Values are like book contents\n",
    "\n",
    "### Example: Attention in Practice\n",
    "\n",
    "For \"The cat sat on the mat\":\n",
    "\n",
    "**Before attention**: Each word has only its own meaning\n",
    "- \"cat\" → [animal, feline, small, ...]\n",
    "- \"sat\" → [action, past tense, positioning, ...]\n",
    "\n",
    "**After attention**: Each word incorporates contextual information  \n",
    "- \"cat\" → [animal, feline, **performed sitting**, **on furniture**, ...]\n",
    "- \"sat\" → [action, **done by cat**, past tense, **on mat**, ...]\n",
    "\n",
    "### The Output: Contextualized Representations\n",
    "\n",
    "The final output is a set of **contextualized embeddings** where each token's representation has been enriched with relevant information from the entire sequence, weighted by attention.\n",
    "\n",
    "This forms the foundation for:\n",
    "- **Language understanding**: Words understand their context\n",
    "- **Compositionality**: Meaning emerges from relationships  \n",
    "- **Long-range dependencies**: Distant words can influence each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED: Implement value aggregation using attention weights\n",
    "# Implemented: Compute the final attention output by aggregating values\n",
    "\n",
    "def aggregate_values(attention_weights, V):\n",
    "    \"\"\"\n",
    "    Aggregate value vectors using attention weights.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "        V: Value tensor (batch_size, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attended output (batch_size, seq_len, d_v)\n",
    "    \"\"\"\n",
    "    # Implementation follows: output = attention_weights @ V\n",
    "    \n",
    "    # Matrix multiplication between attention weights and values\n",
    "    # attention_weights shape: (batch_size, seq_len, seq_len)\n",
    "    # V shape: (batch_size, seq_len, d_v)\n",
    "    # Result shape: (batch_size, seq_len, d_v)\n",
    "    \n",
    "    # For each query position i, compute weighted sum:\n",
    "    # output[i] = sum_j(attention_weights[i,j] * V[j])\n",
    "\n",
    "# Test implementation\n",
    "attended_output = aggregate_values(attention_weights, V)\n",
    "print(f\"Attended output shape: {attended_output.shape}\")\n",
    "print(f\"Original V shape: {V.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(\"Successfully computed attended values!\")\n",
    "print(\"Each token now has a contextualized representation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Attended Values\n",
    "# This will be populated once you implement the function above\n",
    "# visualize_attended_values(attended_output, V, attention_weights, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Complete Attention Mechanism\n",
    "\n",
    "Now let's put it all together into a complete attention function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED: Implement the complete attention mechanism\n",
    "# Implemented: Combine all the steps into one comprehensive function\n",
    "\n",
    "def attention_mechanism(embeddings, d_k=64):\n",
    "    \"\"\"\n",
    "    Complete attention mechanism implementation.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Input embeddings (batch_size, seq_len, d_model)\n",
    "        d_k: Dimension for Q, K, V projections\n",
    "    \n",
    "    Returns:\n",
    "        output: Attended output (batch_size, seq_len, d_k)\n",
    "        attention_weights: Attention weights for visualization\n",
    "    \"\"\"\n",
    "    # Combine all 4 steps into one complete function\n",
    "    \n",
    "    # Step 1: Create Q, K, V projections\n",
    "    \n",
    "    # Step 2: Compute attention scores\n",
    "    \n",
    "    # Step 3: Apply softmax to get attention weights\n",
    "    \n",
    "    # Step 4: Aggregate values using attention weights\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test complete implementation\n",
    "final_output, final_attention_weights = attention_mechanism(embeddings)\n",
    "print(f\"Final output shape: {final_output.shape}\")\n",
    "print(f\"Final attention weights shape: {final_attention_weights.shape}\")\n",
    "print(f\"Input embedding dim: {embeddings.shape[-1]}, Output dim: {final_output.shape[-1]}\")\n",
    "print(\"Complete attention mechanism implemented successfully!\")\n",
    "print(\"The attention mechanism has transformed static embeddings into contextualized representations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Let's evaluate your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of your implementation\n",
    "# This will be populated once you implement the complete attention mechanism\n",
    "evaluation_results = evaluate_attention_output(final_output, final_attention_weights, embeddings)\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture: How Attention Transforms Understanding\n",
    "\n",
    "You have successfully implemented the attention mechanism! Let's connect all the pieces to see the complete picture.\n",
    "\n",
    "### The Four-Step Journey\n",
    "\n",
    "**The attention mechanism solves a fundamental problem**: How can each word in a sentence understand and incorporate information from all other words?\n",
    "\n",
    "1. **Linear Projections (Q, K, V)**: Create three different \"views\" of each word\n",
    "   - Transform static embeddings into dynamic, task-specific representations\n",
    "   - Enable words to express what they need, what they offer, and what they contain\n",
    "\n",
    "2. **Scaled Dot-Product**: Measure compatibility between information needs and offerings\n",
    "   - Quantify relationships through geometric similarity (dot products)\n",
    "   - Scale to maintain stable gradients for effective learning\n",
    "\n",
    "3. **Softmax Normalization**: Convert compatibility into attention allocation  \n",
    "   - Create probability distributions for interpretable attention weights\n",
    "   - Ensure each word allocates exactly 100% of its attention across all positions\n",
    "\n",
    "4. **Value Aggregation**: Gather and combine relevant information\n",
    "   - Perform weighted averaging based on attention decisions\n",
    "   - Create contextualized representations that incorporate global information\n",
    "\n",
    "### The Transformation: From Static to Dynamic\n",
    "\n",
    "**Before Attention** (Static embeddings):\n",
    "```\n",
    "\"The\" → [article, definite, ...]\n",
    "\"cat\" → [animal, feline, small, ...]  \n",
    "\"sat\" → [action, past, positioning, ...]\n",
    "\"on\"  → [preposition, spatial, ...]\n",
    "\"the\" → [article, definite, ...]\n",
    "\"mat\" → [object, flat, surface, ...]\n",
    "```\n",
    "\n",
    "**After Attention** (Contextualized representations):\n",
    "```\n",
    "\"The\" → [article, **refers to cat**, definite, ...]\n",
    "\"cat\" → [animal, **performs sitting**, feline, **subject role**, ...]\n",
    "\"sat\" → [action, **done by cat**, past, **on surface**, ...]  \n",
    "\"on\"  → [preposition, **connects cat and mat**, spatial, ...]\n",
    "\"the\" → [article, **refers to mat**, definite, ...]\n",
    "\"mat\" → [object, **location of sitting**, flat, **receives cat**, ...]\n",
    "```\n",
    "\n",
    "### Key Insights and Implications\n",
    "\n",
    "#### 1. **Parallel Processing**\n",
    "Unlike sequential models (RNNs), attention processes all positions simultaneously:\n",
    "- All words can attend to all other words in one pass\n",
    "- Enables parallelization and faster training\n",
    "- Captures long-range dependencies directly\n",
    "\n",
    "#### 2. **Learned Relationships**  \n",
    "The attention patterns emerge from learning, not hard-coded rules:\n",
    "- Q, K, V projections learn what relationships to look for\n",
    "- Attention weights discover syntactic and semantic patterns\n",
    "- Model learns grammar, syntax, and semantics implicitly\n",
    "\n",
    "#### 3. **Context-Dependent Meaning**\n",
    "Words develop different meanings based on context:\n",
    "- \"bank\" in \"river bank\" vs. \"savings bank\" gets different attended information\n",
    "- Same mechanism handles ambiguity resolution and context integration\n",
    "- Dynamic contextualization at every layer\n",
    "\n",
    "#### 4. **Foundation for Transformers**\n",
    "This attention mechanism is the core building block of:\n",
    "- **BERT**: Bidirectional attention for understanding\n",
    "- **GPT**: Causal (masked) attention for generation  \n",
    "- **T5**: Encoder-decoder attention for translation\n",
    "- **Vision Transformers**: Attention over image patches\n",
    "\n",
    "### Mathematical Elegance\n",
    "\n",
    "The entire mechanism can be expressed in one equation:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "This simple formula encapsulates:\n",
    "- Information retrieval (queries and keys)\n",
    "- Relevance measurement (dot products)\n",
    "- Decision making (softmax)\n",
    "- Information aggregation (weighted values)\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Theoretical Foundation**: Deep understanding of why each component is necessary\n",
    "2. **Mathematical Formulation**: Precise equations and their intuitive meanings  \n",
    "3. **Implementation Skills**: Hands-on experience building attention from scratch\n",
    "4. **Tensor Thinking**: Understanding of shapes, dimensions, and operations\n",
    "5. **Architectural Insight**: How attention enables modern language models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "With this foundation, you're ready to explore:\n",
    "- **Multi-head attention**: Multiple parallel attention mechanisms\n",
    "- **Transformer architecture**: Stacking attention with feedforward layers\n",
    "- **Positional encoding**: Handling sequence order information\n",
    "- **Advanced variants**: Sparse attention, linear attention, and more\n",
    "\n",
    "**Congratulations!** You've mastered one of the most important innovations in modern AI. The attention mechanism you've implemented forms the backbone of today's most powerful language models and continues to drive breakthroughs in artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Educational vs Production Transformers\n",
    "\n",
    "Congratulations! You've now explored how your educational attention implementation relates to production transformer models. Let's consolidate what you've learned.\n",
    "\n",
    "### Key Discoveries\n",
    "\n",
    "Through this comparison, you should have discovered:\n",
    "\n",
    "1. **Core Mechanism Consistency**: Despite the scale differences, both implementations use the same fundamental attention mechanism with softmax normalization\n",
    "\n",
    "2. **Scale Differences**: Production models use ~12x larger embeddings (768D vs 64D) and much more complex architectures (12 heads × 6 layers vs 1 head × 1 layer)\n",
    "\n",
    "3. **Purpose-Driven Design**: Your educational implementation prioritizes clarity and understanding, while production models prioritize performance and expressiveness\n",
    "\n",
    "4. **Mathematical Foundation**: The same mathematical formulas underlie both implementations - the difference is in scale and optimization\n",
    "\n",
    "### Bridging Theory and Practice\n",
    "\n",
    "This comparison demonstrates that:\n",
    "- **Learning fundamentals** prepares you for understanding complex systems\n",
    "- **Educational simplification** doesn't mean losing essential concepts\n",
    "- **Production complexity** builds on simple, well-understood foundations\n",
    "- **Scale matters** for performance but not for core understanding\n",
    "\n",
    "### Next Steps in Your Learning Journey\n",
    "\n",
    "Now that you understand both the fundamentals and the production reality, you're ready to:\n",
    "- Explore transformer variants (BERT, GPT, T5)\n",
    "- Study multi-head attention in detail\n",
    "- Learn about positional encodings and layer normalization\n",
    "- Understand training techniques for large language models\n",
    "- Apply attention mechanisms to other domains (computer vision, speech, etc.)\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "You've mastered one of the most important innovations in modern AI. The attention mechanism forms the backbone of:\n",
    "- **Language Models**: GPT, BERT, T5, and their successors\n",
    "- **Machine Translation**: Google Translate and similar systems\n",
    "- **Computer Vision**: Vision Transformers (ViTs)\n",
    "- **Multimodal Models**: CLIP, DALL-E, and others\n",
    "- **Scientific Computing**: Protein folding, drug discovery, and more\n",
    "\n",
    "**You now understand the core mechanism that powers much of modern AI!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED: Explore dimension adaptation\n",
    "\n",
    "print(\"Exploring dimension adaptation between reference and production scales...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get your reference embeddings from earlier in the notebook\n",
    "print(f\"Starting with reference embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Experiment with dimension adaptation\n",
    "methods = [\"project\", \"pad\", \"truncate\"]\n",
    "target_dim = 768  # Production transformer dimension\n",
    "\n",
    "print(f\"\\nAdapting from {embeddings.shape[-1]}D to {target_dim}D:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Loop through each method and test adaptation\n",
    "for method in methods:\n",
    "    print(f\"\\nMethod: {method.upper()}\")\n",
    "    \n",
    "    try:\n",
    "        adapted = adapt_dimensions(embeddings, target_dim, method=method)\n",
    "        print(f\"   Original: {embeddings.shape}\")\n",
    "        print(f\"   Adapted:  {adapted.shape}\")\n",
    "        print(f\"   Successfully adapted using {method} method\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error with {method} method: {e}\")\n",
    "\n",
    "# Test reverse adaptation (Production → Reference)\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"Testing reverse adaptation (Production → Reference)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "reference_dim = 64\n",
    "# Create a simulated production tensor\n",
    "production_tensor = torch.randn(1, 6, 768)\n",
    "print(f\"Production tensor shape: {production_tensor.shape}\")\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\nReverse Method: {method.upper()}\")\n",
    "    try:\n",
    "        reverse_adapted = adapt_dimensions(production_tensor, reference_dim, method=method)\n",
    "        print(f\"   Production: {production_tensor.shape}\")\n",
    "        print(f\"   Adapted:    {reverse_adapted.shape}\")\n",
    "        print(f\"   Successfully adapted from 768D to 64D using {method}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error with reverse {method}: {e}\")\n",
    "\n",
    "print(f\"\\nREFLECTION:\")\n",
    "print(\"1. Project method creates learned transformations\")\n",
    "print(\"2. Pad method preserves original information but adds zeros\")\n",
    "print(\"3. Truncate method may lose information but is simple\")\n",
    "print(\"4. Each method has different computational and information trade-offs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Adaptation: Bridging the Gap\n",
    "\n",
    "One challenge when working with both educational and production models is handling the dimension mismatch. Your reference uses 64D embeddings while production models use 768D+. Let's explore how to bridge this gap.\n",
    "\n",
    "### Why Dimension Adaptation Matters\n",
    "\n",
    "- **Integration**: Combining insights from both implementations\n",
    "- **Visualization**: Adapting production outputs for educational visualization\n",
    "- **Experimentation**: Testing ideas across different scales\n",
    "- **Understanding**: Seeing how dimensional choices affect model behavior\n",
    "\n",
    "### Adaptation Methods\n",
    "\n",
    "1. **Projection**: Linear transformation (learns optimal mapping)\n",
    "2. **Padding**: Adding zeros (preserves original information)\n",
    "3. **Truncation**: Simple reduction (may lose information)\n",
    "\n",
    "### TODO: Experiment with Adaptation\n",
    "\n",
    "Consider these questions as you explore:\n",
    "\n",
    "1. **Which method** do you think would work best for different use cases?\n",
    "2. **What information** might be lost when reducing dimensions?\n",
    "3. **How might** different adaptation methods affect attention patterns?\n",
    "4. **When would you** want to go from 64D to 768D vs 768D to 64D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize model comparison\n",
    "\n",
    "print(\"Creating visual comparison of implementations...\")\n",
    "print(\"Note: This creates a comprehensive 2x2 subplot showing key differences\")\n",
    "\n",
    "# Use the visualize_model_comparison() function\n",
    "if 'comparison_results' in locals() and comparison_results is not None:\n",
    "    try:\n",
    "        visualize_model_comparison(comparison_results)\n",
    "        print(\"Visualization created successfully!\")\n",
    "        print(\"Compare the attention patterns, dimensions, and architecture complexity.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")\n",
    "        print(\"This may be due to matplotlib backend issues in some environments\")\n",
    "else:\n",
    "    print(\"No comparison results available. Please run the comparison cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison\n",
    "\n",
    "Let's create visualizations that show the differences between your reference implementation and the production transformer. This will help you see both the similarities and differences at a glance.\n",
    "\n",
    "### What These Visualizations Show\n",
    "\n",
    "1. **Attention Weight Heatmaps**: Side-by-side comparison of attention patterns\n",
    "2. **Embedding Dimension Comparison**: Visual representation of the scale difference\n",
    "3. **Architecture Complexity**: Comparison of model complexity metrics\n",
    "4. **Parameter Count Visualization**: Understanding the computational requirements\n",
    "\n",
    "These visualizations make abstract concepts concrete and help bridge the gap between educational simplicity and production complexity.\n",
    "\n",
    "### TODO: Interpret the Visualizations\n",
    "\n",
    "Before creating the visualizations, predict what you'll see:\n",
    "\n",
    "1. **Attention Patterns**: Will they look similar or different? Why?\n",
    "2. **Dimension Scale**: How will the 12x size difference appear visually?\n",
    "3. **Complexity**: How can we visualize the difference between 1 head vs 12 heads?\n",
    "4. **Core Mechanism**: What should remain consistent between both implementations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare your implementation with production transformer\n",
    "\n",
    "print(\"Running side-by-side comparison...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the comprehensive comparison using our example text\n",
    "try:\n",
    "    comparison_results = compare_attention_implementations(PROMPT_EXAMPLE)\n",
    "    \n",
    "    print(\"\\nComparison Summary:\")\n",
    "    print(f\"Reference embedding dimension: {comparison_results['comparison']['embedding_dimensions']['reference']}\")\n",
    "    print(f\"Transformer embedding dimension: {comparison_results['comparison']['embedding_dimensions']['transformer']}\")\n",
    "    print(f\"Dimension ratio (production/reference): {comparison_results['comparison']['embedding_dimensions']['ratio']:.1f}x\")\n",
    "    print(f\"Both use proper attention normalization: {comparison_results['comparison']['attention_patterns']['both_sum_to_one']}\")\n",
    "    \n",
    "    print(\"\\nKey Educational Insights:\")\n",
    "    for insight in comparison_results['educational_insights']:\n",
    "        print(f\"• {insight}\")\n",
    "    \n",
    "    print(\"\\nComparison complete! Ready for visualization.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Comparison failed: {e}\")\n",
    "    print(\"This might be due to missing model loading or implementation issues\")\n",
    "    comparison_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Comparison\n",
    "\n",
    "Now let's run both your implementation and the production transformer on the same input text and compare their approaches, outputs, and architectural differences.\n",
    "\n",
    "### What We're Comparing\n",
    "\n",
    "1. **Input Processing**: How each model tokenizes and embeds our example text\n",
    "2. **Attention Computation**: Single-head vs multi-head attention patterns  \n",
    "3. **Architectural Scale**: Dimensions, layers, and complexity differences\n",
    "4. **Output Analysis**: How the final representations differ\n",
    "\n",
    "### The Comparison Framework\n",
    "\n",
    "The comparison function will show:\n",
    "- **Quantitative differences**: Embedding dimensions, parameter counts, layer depths\n",
    "- **Qualitative similarities**: Both use softmax normalization, attention weights sum to 1\n",
    "- **Educational insights**: Why production models need more complexity\n",
    "\n",
    "### TODO: Analyze the Comparison\n",
    "\n",
    "Think about these questions before running the comparison:\n",
    "\n",
    "1. **What do you predict** will be the biggest difference between implementations?\n",
    "2. **What do you expect** to remain the same (core attention mechanism)?\n",
    "3. **How might the scale difference** affect the attention patterns?\n",
    "4. **Why would production models** need 12x larger embeddings than your implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load production transformer for comparison\n",
    "\n",
    "print(\"Loading production transformer model...\")\n",
    "print(\"Note: This requires internet connection for first-time download\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Call the load_mini_transformer() function\n",
    "try:\n",
    "    model, tokenizer = load_mini_transformer()\n",
    "    print(\"Successfully loaded DistilGPT-2!\")\n",
    "    print(f\"Model type: {model.config.model_type}\")\n",
    "    print(f\"Embedding dimension: {model.config.hidden_size}\")\n",
    "    print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n",
    "    print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "    print(\"Ready for comparison with our reference implementation!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load transformer: {e}\")\n",
    "    print(\"This may be due to:\")\n",
    "    print(\"- Missing 'transformers' library\")\n",
    "    print(\"- No internet connection for first download\")\n",
    "    print(\"- Network/firewall restrictions\")\n",
    "    print(\"Don't worry - we can still explore the concepts conceptually!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading: Production Transformer\n",
    "\n",
    "First, let's load a small production transformer model for comparison. We'll use DistilGPT-2, which is a smaller, faster version of GPT-2 that still demonstrates production-level architecture.\n",
    "\n",
    "### Why DistilGPT-2?\n",
    "\n",
    "- **Size**: ~82M parameters (manageable for educational purposes)\n",
    "- **Architecture**: Real transformer with multi-head attention\n",
    "- **Performance**: Fast enough for interactive exploration\n",
    "- **Accessibility**: Free and widely available through HuggingFace\n",
    "\n",
    "### TODO: Explore Model Loading\n",
    "\n",
    "Run the cell below to load the production transformer. While it's loading, think about:\n",
    "\n",
    "1. **What differences do you expect** between your implementation and a production model?\n",
    "2. **Why might production models** need more complexity than your educational version?\n",
    "3. **How can we meaningfully compare** implementations of different scales?\n",
    "\n",
    "**Note**: The first run requires internet connection to download the model (~350MB). Subsequent runs use cached files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the transformer integration functions\n",
    "from src.model_utils import (\n",
    "    load_mini_transformer,\n",
    "    compare_attention_implementations,\n",
    "    visualize_model_comparison,\n",
    "    adapt_dimensions\n",
    ")\n",
    "\n",
    "print(\"Transformer integration functions loaded!\")\n",
    "print(\"Ready to compare with production transformers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Transformer Model Comparison\n",
    "\n",
    "## From Educational Implementation to Production Reality\n",
    "\n",
    "Now that you understand how attention works from first principles, let's see how your implementation compares to real-world production transformers. This section bridges the gap between learning and practical application.\n",
    "\n",
    "### The Scale Gap: Education vs Production\n",
    "\n",
    "Your reference implementation was designed for **clarity and understanding**:\n",
    "- 64-dimensional embeddings (easy to visualize and debug)\n",
    "- Single attention head (focus on core mechanism)\n",
    "- One attention computation (minimal complexity)\n",
    "- Educational example: \"The cat sat on the mat\"\n",
    "\n",
    "Production transformers prioritize **performance and expressiveness**:\n",
    "- 768+ dimensional embeddings (rich representation space)\n",
    "- 12+ attention heads (multiple perspectives on relationships)\n",
    "- 6-12+ layers (deep hierarchical processing)\n",
    "- Complex tokenization and vocabulary handling\n",
    "\n",
    "### Key Questions This Section Answers\n",
    "\n",
    "1. **Scale**: How much larger are production models compared to your reference?\n",
    "2. **Architecture**: What additional complexity do production models add?\n",
    "3. **Consistency**: Do production models use the same core attention mechanism?\n",
    "4. **Performance**: Why do production models need so much more complexity?\n",
    "\n",
    "### Your Learning Journey\n",
    "\n",
    "This comparison helps you:\n",
    "- **Appreciate the fundamentals**: The core mechanism remains the same\n",
    "- **Understand complexity**: See why production models are more sophisticated\n",
    "- **Bridge theory to practice**: Connect academic understanding to real applications\n",
    "- **Gain perspective**: Recognize what scales and what stays constant\n",
    "\n",
    "Let's explore these differences hands-on!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
